{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ada9594",
   "metadata": {},
   "source": [
    "### UNet Model for Image Segmentation with Data Loading and Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d4d497",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from albumentations import Compose, RandomRotate90, Flip, Transpose\n",
    "from torchvision import transforms\n",
    "import cv2\n",
    "\n",
    "train = pd.read_csv('kvasir-instrument/train.txt')\n",
    "test = pd.read_csv('kvasir-instrument/test.txt')\n",
    "\n",
    "# Printing the number of samples in the training and test datasets\n",
    "\n",
    "print(f'Total training samples: {len(train)}')\n",
    "print(f'Total test samples: {len(test)}')\n",
    "\n",
    "# Defining Augmentation transformations\n",
    "aug_transform = Compose([\n",
    "    RandomRotate90(),\n",
    "    Flip(),\n",
    "    Transpose(),\n",
    "])\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, transform=None, target_size=(256, 256)):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "        self.target_size = target_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name = self.data.values[idx][0]\n",
    "        img_name = 'kvasir-instrument/images/images/' + name + '.jpg'\n",
    "        img_mask = 'kvasir-instrument/masks/masks/' + name + '.png'\n",
    "\n",
    "        img = Image.open(img_name).convert('RGB')\n",
    "        mask = Image.open(img_mask).convert('L')\n",
    "        \n",
    "        # Resizing images and masks to the target size\n",
    "        img = img.resize(self.target_size, Image.Resampling.BILINEAR)\n",
    "        mask = mask.resize(self.target_size, Image.Resampling.NEAREST)\n",
    "\n",
    "        # Applying transformations\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=np.array(img), mask=np.array(mask))\n",
    "            img = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "        \n",
    "        # Converting PIL images to tensors\n",
    "        img = transforms.ToTensor()(img)\n",
    "        mask = transforms.ToTensor()(mask)\n",
    "        \n",
    "        return img, mask\n",
    "\n",
    "train_dataset = CustomDataset(train, transform=aug_transform)\n",
    "test_dataset = CustomDataset(test, transform=None)  # No augmentation since its not needed for test dataset\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UNet, self).__init__()\n",
    "        \n",
    "        # Encoder (contracting path)\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        # Middle (bottleneck)\n",
    "        self.middle = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        # Decoder (expansive path)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(64, out_channels, kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.encoder(x)\n",
    "        x2 = self.middle(x1)\n",
    "        x3 = self.decoder(x2)\n",
    "        return x3\n",
    "\n",
    "# Initialising the model, loss function, and optimizer\n",
    "in_channels = 3  \n",
    "out_channels = 1  \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  \n",
    "model = UNet(in_channels, out_channels).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()   \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70e5d04",
   "metadata": {},
   "source": [
    "### UNet Model Training and Testing Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c39ffc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "print(\"Training UNetModel:\")\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for i, (images, masks) in enumerate(train_loader):\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Resizing the output to match the mask size\n",
    "        outputs = nn.functional.interpolate(outputs, size=masks.size()[2:], mode='bilinear', align_corners=True)\n",
    "        \n",
    "        loss = criterion(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    average_loss = total_loss / len(train_loader)\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Average Loss: {average_loss:.4f}')\n",
    "print('Finished Training')\n",
    "\n",
    "\n",
    "\n",
    "torch.save(model.state_dict(), 'unet_model.pth')\n",
    "print('Trained model saved successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae84f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing loop\n",
    "print(\"Testing UNetModel:\")\n",
    "model.eval()\n",
    "total_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for i, (images, masks) in enumerate(test_loader):\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Resizing the output to match the mask size\n",
    "        outputs = nn.functional.interpolate(outputs, size=masks.size()[2:], mode='bilinear', align_corners=True)\n",
    "        \n",
    "        loss = criterion(outputs, masks)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "average_loss = total_loss / len(test_loader)\n",
    "print(f'Average Test Loss: {average_loss:.4f}')\n",
    "print('Finished Testing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7923669",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Provided training loss values\n",
    "epoch_numbers = list(range(1, 11))\n",
    "training_loss_values = [0.2884, 0.1682, 0.1488, 0.1507, 0.1495, 0.1450, 0.1358, 0.1334, 0.1338, 0.1332]\n",
    "\n",
    "# Plotting the training loss over epochs\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epoch_numbers, training_loss_values, color='blue', marker='o')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Training Loss')\n",
    "plt.title('Training Loss of UNet Model over Epochs')\n",
    "plt.grid(True)\n",
    "plt.xticks(epoch_numbers)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6244996",
   "metadata": {},
   "source": [
    "### Model Summary with Layer-wise Input and Output Shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907d9c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary(model, input_size):\n",
    "    def forward_hook(module, input, output):\n",
    "        # Printing layer information\n",
    "        print(f\"{module.__class__.__name__.ljust(20)} | \"\n",
    "              f\"Input shape: {str(input[0].shape).ljust(30)} | \"\n",
    "              f\"Output shape: {str(output.shape).ljust(30)}\")\n",
    "\n",
    "    # Registering the hook to each layer\n",
    "    hooks = []\n",
    "    for layer in model.children():\n",
    "        hook = layer.register_forward_hook(forward_hook)\n",
    "        hooks.append(hook)\n",
    "\n",
    "    # Generating a dummy input\n",
    "    input_tensor = torch.rand(input_size).unsqueeze(0)\n",
    "\n",
    "    # Performing a forward pass to print the summary\n",
    "    print(\"\\nModel Summary:\")\n",
    "    print(\"=\" * 75)\n",
    "    model(input_tensor)\n",
    "\n",
    "    # Removing the hooks\n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "\n",
    "input_size = (in_channels, 256, 256)\n",
    "summary(model, input_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42613ff",
   "metadata": {},
   "source": [
    "### Model Evaluation Metrics: \n",
    "##### Test Loss and Intersection over Union (IoU) with Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c99ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, jaccard_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to calculate metrics\n",
    "def calculate_metrics(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_predictions = []\n",
    "    all_masks = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (images, masks) in enumerate(dataloader):\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            outputs = nn.functional.interpolate(outputs, size=masks.size()[2:], mode='bilinear', align_corners=True)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, masks)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Converting to binary predictions\n",
    "            predictions = torch.sigmoid(outputs) > 0.5\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_masks.extend(masks.cpu().numpy())\n",
    "\n",
    "    average_loss = total_loss / len(dataloader)\n",
    "\n",
    "    return average_loss, np.array(all_predictions), np.array(all_masks)\n",
    "\n",
    "# Calculating metrics on the test set\n",
    "test_loss, test_predictions, test_masks = calculate_metrics(model, test_loader, criterion)\n",
    "\n",
    "# Calculating IoU\n",
    "iou = jaccard_score(test_masks.flatten(), test_predictions.flatten(), average='micro')\n",
    "\n",
    "# Print\n",
    "print(f'Test Loss: {test_loss:.4f}')\n",
    "print(f'Intersection over Union (IoU): {iou:.4f}')\n",
    "\n",
    "# Visualise the results using bar plots\n",
    "metrics_names = ['Test Loss', 'Intersection over Union (IoU)']\n",
    "metrics_values = [test_loss, iou]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(metrics_names, metrics_values, color=['teal', 'salmon'])\n",
    "plt.title('Evaluation Metrics')\n",
    "plt.xlabel('Metrics')\n",
    "plt.ylabel('Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c50e25b",
   "metadata": {},
   "source": [
    "### Evaluation Metrics for U-Net Semantic Segmentation: \n",
    "##### Confusion Matrix, Precision-Recall Curve, ROC Curve, and Dice Coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b584c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, roc_curve, auc\n",
    "from sklearn.metrics import f1_score, average_precision_score\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from torchvision import utils\n",
    "\n",
    "# Loading the trained model\n",
    "model = UNet(in_channels, out_channels).to(device)\n",
    "model.load_state_dict(torch.load('unet_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Initialising empty lists to store predictions and true labels\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, masks in test_loader:\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Converting the model output to binary predictions based on a threshold\n",
    "        predictions = (torch.sigmoid(outputs) > 0.5).cpu().numpy()\n",
    "        \n",
    "        # Converting true labels to numpy array\n",
    "        labels = masks.cpu().numpy()\n",
    "        \n",
    "        # Appending predictions and true labels to the lists\n",
    "        all_predictions.append(predictions)\n",
    "        all_labels.append(labels)\n",
    "\n",
    "all_predictions = np.concatenate(all_predictions)\n",
    "all_labels = np.concatenate(all_labels)\n",
    "\n",
    "# Flatten the predictions and true labels\n",
    "flat_predictions = all_predictions.flatten()\n",
    "flat_labels = all_labels.flatten()\n",
    "\n",
    "# Both arrays have the same shape\n",
    "min_length = min(len(flat_predictions), len(flat_labels))\n",
    "flat_predictions = flat_predictions[:min_length]\n",
    "flat_labels = flat_labels[:min_length]\n",
    "\n",
    "# 1. Confusion Matrix\n",
    "conf_matrix = confusion_matrix(flat_labels, flat_predictions)\n",
    "\n",
    "\n",
    "# 2. Precision-Recall Curve\n",
    "precision, recall, _ = precision_recall_curve(flat_labels, flat_predictions)\n",
    "average_precision = average_precision_score(flat_labels, flat_predictions)\n",
    "\n",
    "# 3. ROC Curve\n",
    "fpr, tpr, _ = roc_curve(flat_labels, flat_predictions)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# 4. Dice Coefficient\n",
    "intersection = np.sum(flat_predictions * flat_labels)\n",
    "dice_coefficient = (2.0 * intersection) / (np.sum(flat_predictions) + np.sum(flat_labels))\n",
    "\n",
    "# Displaying\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "print(f\"Average Precision: {average_precision:.4f}\")\n",
    "print(f\"Area under ROC Curve: {roc_auc:.4f}\")\n",
    "print(f\"Dice Coefficient: {dice_coefficient:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8cded8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plottin Precision-Recall Curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall, precision, color='blue', lw=2, label='Precision-Recall Curve (AP={:.2f})'.format(average_precision))\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "# Plotting ROC Curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC Curve (AUC={:.2f})'.format(roc_auc))\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a8b676",
   "metadata": {},
   "source": [
    "### Visualisation Feature Maps in a Convolutional Layer of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe05209",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, layer in model.named_modules():\n",
    "    if isinstance(layer, nn.Conv2d):\n",
    "        # Choosing a specific layer to visualize\n",
    "        if layer.in_channels == 3:\n",
    "            activation = layer(images)\n",
    "            num_feature_maps = activation.size(1)\n",
    "\n",
    "            # Number of rows and columns in the grid\n",
    "            rows = num_feature_maps // 8 + 1\n",
    "            columns = min(8, num_feature_maps)\n",
    "\n",
    "            plt.figure(figsize=(16, 2 * rows))\n",
    "            plt.suptitle(f'Feature Maps in Layer: {name}', y=1.02)\n",
    "\n",
    "            for i in range(num_feature_maps):\n",
    "                plt.subplot(rows, columns, i + 1)\n",
    "                plt.imshow(activation[0, i].cpu().detach().numpy(), cmap='viridis')\n",
    "                plt.axis('off')\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            break  # Stopping after visualisation the first convolutional layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17915783",
   "metadata": {},
   "source": [
    "### Model Prediction Visualisation on Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38a32fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import utils\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "# Defining a new dataset for visualisation to compare\n",
    "visualisation_dataset = CustomDataset(train, transform=None)  # No augmentation for visualization\n",
    "visualisation_loader = DataLoader(visualization_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# Visualising images using the trained model\n",
    "with torch.no_grad():\n",
    "    for i, (images, masks) in enumerate(visualization_loader):\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "        # Display original images\n",
    "        plt.figure(figsize=(15, 6))\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.title('Original Image')\n",
    "        plt.imshow(utils.make_grid(images.cpu(), nrow=4).permute(1, 2, 0))\n",
    "\n",
    "        # Applying the trained model to get predictions\n",
    "        outputs = model(images)\n",
    "        outputs = nn.functional.interpolate(outputs, size=masks.size()[2:], mode='bilinear', align_corners=True)\n",
    "\n",
    "        # Converting the model output and mask to binary predictions\n",
    "        predictions = torch.sigmoid(outputs) > 0.5\n",
    "\n",
    "        # Display model predictions\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.title('Model Prediction')\n",
    "        plt.imshow(utils.make_grid(predictions.cpu(), nrow=4).permute(1, 2, 0)[:, :, 0], cmap='gray')\n",
    "\n",
    "        # Display masks\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.title('Mask')\n",
    "        plt.imshow(utils.make_grid(masks.cpu(), nrow=4).permute(1, 2, 0)[:, :, 0], cmap='gray')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "        if i == 0:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912762ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualisation 3 images using the trained model\n",
    "with torch.no_grad():\n",
    "    for i, (images, masks) in enumerate(visualization_loader):\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "        # Displaying original images, model predictions, and masks in separate rows\n",
    "        for j in range(min(3, images.size(0))):\n",
    "            plt.figure(figsize=(8, 4))\n",
    "\n",
    "            # Original image\n",
    "            plt.subplot(1, 3, 1)\n",
    "            plt.title('Original Image')\n",
    "            plt.imshow(to_pil_image(images[j].cpu()))\n",
    "\n",
    "            # Applying the trained model to get predictions\n",
    "            outputs = model(images[j].unsqueeze(0))\n",
    "            outputs = nn.functional.interpolate(outputs, size=masks.size()[2:], mode='bilinear', align_corners=True)\n",
    "\n",
    "            # Converting the model output and mask to numpy arrays\n",
    "            predictions = torch.sigmoid(outputs).cpu().squeeze(0).numpy()\n",
    "            mask_np = masks[j].cpu().numpy()\n",
    "\n",
    "            # Displaying model predictions\n",
    "            plt.subplot(1, 3, 2)\n",
    "            plt.title('Model Prediction')\n",
    "            plt.imshow(predictions[0], cmap='gray', vmin=0, vmax=1)  # Assuming predictions is a single-channel image\n",
    "\n",
    "            # Displaying mask\n",
    "            plt.subplot(1, 3, 3)\n",
    "            plt.title('Mask')\n",
    "            plt.imshow(mask_np[0], cmap='gray', vmin=0, vmax=1)  # Assuming mask_np is a single-channel image\n",
    "\n",
    "            plt.show()\n",
    "\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ca1eab",
   "metadata": {},
   "source": [
    "### Visualisation of Model Predictions on Test Images and Augmented Versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbd274c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing loop with augmentation visualisation\n",
    "print(\"Testing UNetModel:\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, (images, masks) in enumerate(test_loader):\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "        # Original image\n",
    "        original_output = model(images)\n",
    "        original_output = nn.functional.interpolate(original_output, size=masks.size()[2:], mode='nearest')\n",
    "\n",
    "        # Augmented images\n",
    "        augmented_outputs = []\n",
    "        for transform_name, transform_func in zip(['Rotate', 'Flip', 'Transpose'],\n",
    "                                                 [aug_transform.transforms[0], aug_transform.transforms[1], aug_transform.transforms[2]]):\n",
    "            # Applying the transformation to each image in the batch, including the original image\n",
    "            augmented_images = torch.stack([torch.from_numpy(transform_func(image=img.permute(1, 2, 0).cpu().numpy())['image']).permute(2, 0, 1) for img in images], dim=0)\n",
    "\n",
    "            augmented_output = model(augmented_images)\n",
    "            augmented_output = nn.functional.interpolate(augmented_output, size=masks.size()[2:], mode='nearest')\n",
    "            augmented_outputs.append((transform_name, augmented_output))\n",
    "\n",
    "        # Plotting\n",
    "        num_plots = len(augmented_outputs) + 3  # 3 for the original image and prediction\n",
    "        plt.figure(figsize=(5 * num_plots, 5))\n",
    "\n",
    "        # Plotting the original image\n",
    "        plt.subplot(1, num_plots, 1)\n",
    "        plt.title('Original Image')\n",
    "        plt.imshow((images[0].permute(1, 2, 0).cpu().numpy() * 255).astype('uint8'), cmap='gray_r', interpolation='nearest')\n",
    "        plt.axis('off')\n",
    "\n",
    "        # Plotting the model's prediction for the original image\n",
    "        plt.subplot(1, num_plots, 2)\n",
    "        plt.title('Original Prediction')\n",
    "        plt.imshow((torch.sigmoid(original_output[0, 0]).cpu().numpy() * 255).astype('uint8'), cmap='gray_r', interpolation='nearest')\n",
    "        plt.axis('off')\n",
    "\n",
    "        # Plotting the augmented images and their predictions\n",
    "        for j, (transform_name, augmented_output) in enumerate(augmented_outputs):\n",
    "            plt.subplot(1, num_plots, j + 3)\n",
    "            plt.title(f'{transform_name} Augmented Prediction')\n",
    "            plt.imshow((torch.sigmoid(augmented_output[0, 0]).cpu().numpy() * 255).astype('uint8'), cmap='gray_r', interpolation='nearest')\n",
    "            plt.axis('off')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "        if i == 2:  \n",
    "            break  # Break after visualising a 3 samples\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
